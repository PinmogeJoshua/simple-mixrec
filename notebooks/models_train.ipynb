{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 特征工程优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 加载已预处理的数据\n",
    "df = pd.read_csv('../datasets/walmart_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 基于EDA添加更多交互特征\n",
    "# 从热力图可以看出有强相关性的特征\n",
    "df['Age_City'] = df['Age'].astype(str) + \"_\" + df['City_Category']\n",
    "df['Gender_City'] = df['Gender'] + \"_\" + df['City_Category']\n",
    "df['Age_Marital'] = df['Age'].astype(str) + \"_\" + df['Marital_Status'].astype(str)\n",
    "\n",
    "# 2. 基于RFM分析添加客户价值分段\n",
    "# R(Recency)：停留年限的倒数\n",
    "df['Recency_Score'] = 5 - df['Stay_Years']\n",
    "# F(Frequency)：购买频次分段\n",
    "df['Frequency_Score'] = pd.qcut(df['Purchase_Count'], 5, labels=[1, 2, 3, 4, 5])\n",
    "# M(Monetary)：总购买金额分段\n",
    "df['Monetary_Score'] = pd.qcut(df['Total_User_Purchase'], 5, labels=[1, 2, 3, 4, 5])\n",
    "# 综合RFM得分\n",
    "df['RFM_Score'] = df['Recency_Score'].astype(str) + df['Frequency_Score'].astype(str) + df['Monetary_Score'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 产品类别的高阶特征\n",
    "# 按产品类别的购买方差\n",
    "product_var = df.groupby('Product_Category')['Purchase'].var().reset_index()\n",
    "product_var.columns = ['Product_Category', 'Product_Purchase_Var']\n",
    "df = pd.merge(df, product_var, on='Product_Category', how='left')\n",
    "\n",
    "# 4. 城市和职业的交互\n",
    "city_occupation = df.groupby(['City_Category', 'Occupation'])['Purchase'].mean().reset_index()\n",
    "city_occupation.columns = ['City_Category', 'Occupation', 'City_Occupation_Mean']\n",
    "df = pd.merge(df, city_occupation, on=['City_Category', 'Occupation'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 用户购买习惯特征\n",
    "# 用户偏好的产品类别\n",
    "user_category_pref = df.groupby(['User_ID', 'Product_Category']).size().reset_index(name='Category_Purchase_Count')\n",
    "user_max_category = user_category_pref.loc[user_category_pref.groupby('User_ID')['Category_Purchase_Count'].idxmax()]\n",
    "user_max_category.columns = ['User_ID', 'Preferred_Category', 'Category_Count']\n",
    "df = pd.merge(df, user_max_category[['User_ID', 'Preferred_Category']], on='User_ID', how='left')\n",
    "\n",
    "# 6. 创建购买频率和金额的比率特征\n",
    "df['Avg_Transaction_Value'] = df['Total_User_Purchase'] / df['Purchase_Count']\n",
    "\n",
    "# 7. 对一些特征进行多项式转换\n",
    "df['Product_Category_Squared'] = df['Product_Category'] ** 2\n",
    "df['Occupation_Squared'] = df['Occupation'] ** 2\n",
    "\n",
    "# 8. 对数转换购买金额 (目标变量)\n",
    "df['Purchase_Log'] = np.log1p(df['Purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备最终特征集\n",
    "categorical_features = ['Gender', 'Age', 'City_Category', 'Marital_Status', \n",
    "                       'Gender_City', 'Age_City', 'Age_Marital', 'RFM_Score', \n",
    "                       'Preferred_Category']\n",
    "\n",
    "numerical_features = ['Occupation', 'Stay_Years', 'Product_Category',\n",
    "                     'City_Code', 'Gender_Code', 'Total_User_Purchase',\n",
    "                     'Purchase_Count', 'Avg_User_Purchase', 'Avg_Category_Purchase',\n",
    "                     'Product_Purchase_Var', 'City_Occupation_Mean',\n",
    "                     'Avg_Transaction_Value', 'Product_Category_Squared',\n",
    "                     'Occupation_Squared', 'Recency_Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "\n",
    "# 准备X和y\n",
    "target = 'Purchase'  # 或使用'Purchase_Log'如果您想预测log转换后的购买金额\n",
    "X = df.drop(['Purchase', 'Purchase_Log'], axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# 去除不需要的列\n",
    "cols_to_drop = ['User_ID', 'Product_ID', 'Stay_In_Current_City_Years', 'Purchase_Normalized',\n",
    "                'Purchase_Standardized', 'Age_Category']\n",
    "X = X.drop([col for col in cols_to_drop if col in X.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的数值特征: ['Occupation', 'Stay_Years', 'Product_Category', 'City_Code', 'Gender_Code', 'Total_User_Purchase', 'Purchase_Count', 'Avg_User_Purchase', 'Avg_Category_Purchase', 'Product_Purchase_Var', 'City_Occupation_Mean', 'Avg_Transaction_Value', 'Product_Category_Squared', 'Occupation_Squared', 'Recency_Score']\n",
      "使用的分类特征: ['Gender', 'Age', 'City_Category', 'Marital_Status', 'Gender_City', 'Age_City', 'Age_Marital', 'RFM_Score', 'Preferred_Category']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 预处理管道\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "available_num_features = [col for col in numerical_features if col in X_train.columns]\n",
    "available_cat_features = [col for col in categorical_features if col in X_train.columns]\n",
    "\n",
    "print(\"使用的数值特征:\", available_num_features)\n",
    "print(\"使用的分类特征:\", available_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, [col for col in numerical_features if col in X.columns]),\n",
    "        ('cat', categorical_transformer, [col for col in categorical_features if col in X.columns])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理后特征数量: 149\n"
     ]
    }
   ],
   "source": [
    "preprocessed_X_train = preprocessor.fit_transform(X_train)\n",
    "preprocessed_X_test = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"预处理后特征数量: {preprocessed_X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 基础模型构建与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建基础模型\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=50, n_jobs=-1, random_state=42)),\n",
    "    ('gbm', GradientBoostingRegressor(n_estimators=50, random_state=42)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=50, n_jobs=-1, random_state=42)),\n",
    "    ('lgbm', lgb.LGBMRegressor(n_estimators=50, n_jobs=-1, random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集总大小: 440054行\n",
      "数据集较大，使用44005个样本(约10.0%的数据)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 动态确定样本大小\n",
    "total_size = len(X_train)\n",
    "print(f\"训练集总大小: {total_size}行\")\n",
    "\n",
    "# 根据数据集大小自动确定样本大小\n",
    "if total_size < 5000:\n",
    "    # 如果数据集很小，使用全部数据\n",
    "    sample_size = total_size\n",
    "    print(f\"数据集较小，使用全部{sample_size}个样本\")\n",
    "elif total_size < 50000:\n",
    "    # 中等大小数据集，使用20%或5000个样本(取较大值)\n",
    "    sample_size = max(int(total_size * 0.2), 5000)\n",
    "    print(f\"数据集中等大小，使用{sample_size}个样本(约{sample_size/total_size*100:.1f}%的数据)\")\n",
    "else:\n",
    "    # 大型数据集，使用10%或10000个样本(取较大值)\n",
    "    sample_size = max(int(total_size * 0.1), 10000)\n",
    "    print(f\"数据集较大，使用{sample_size}个样本(约{sample_size/total_size*100:.1f}%的数据)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从440054行数据中抽取44005行用于快速模型评估\n",
      "rf (样本训练) - R²: 0.6239, RMSE: 3074.26, 训练时间: 16.87秒\n",
      "gbm (样本训练) - R²: 0.6712, RMSE: 2874.09, 训练时间: 5.25秒\n",
      "xgb (样本训练) - R²: 0.6708, RMSE: 2875.86, 训练时间: 0.18秒\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1472\n",
      "[LightGBM] [Info] Number of data points in the train set: 44005, number of used features: 142\n",
      "[LightGBM] [Info] Start training from score 9267.369549\n",
      "lgbm (样本训练) - R²: 0.6738, RMSE: 2863.01, 训练时间: 0.13秒\n"
     ]
    }
   ],
   "source": [
    "# 如果有足够的样本，则取样本进行训练\n",
    "if sample_size < total_size:\n",
    "    sample_indices = np.random.choice(total_size, sample_size, replace=False)\n",
    "    sample_X_train = preprocessed_X_train[sample_indices]\n",
    "    sample_y_train = y_train.iloc[sample_indices]\n",
    "    \n",
    "    print(f\"从{total_size}行数据中抽取{sample_size}行用于快速模型评估\")\n",
    "    \n",
    "    # 使用样本数据训练模型\n",
    "    for name, model in base_models:\n",
    "        start_time = time.time()\n",
    "        model.fit(sample_X_train, sample_y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        y_pred = model.predict(preprocessed_X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        print(f\"{name} (样本训练) - R²: {r2:.4f}, RMSE: {rmse:.2f}, 训练时间: {train_time:.2f}秒\")\n",
    "else:\n",
    "    # 数据集很小，使用全部数据\n",
    "    print(\"数据集较小，使用全部数据进行训练\")\n",
    "    \n",
    "    for name, model in base_models:\n",
    "        start_time = time.time()\n",
    "        model.fit(preprocessed_X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        y_pred = model.predict(preprocessed_X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        print(f\"{name} - R²: {r2:.4f}, RMSE: {rmse:.2f}, 训练时间: {train_time:.2f}秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 随机森林模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [200, 300, 400],\n",
    "    'model__max_depth': [None, 15, 20, 25],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型调参\n",
    "grid_search = RandomizedSearchCV(\n",
    "    rf_pipeline, \n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5, \n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在样本数据上进行参数搜索...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "最佳参数:\n",
      "{'model__n_estimators': 400, 'model__min_samples_split': 10, 'model__min_samples_leaf': 4, 'model__max_features': 'sqrt', 'model__max_depth': 20}\n"
     ]
    }
   ],
   "source": [
    "sample_indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_train_sample = X_train.iloc[sample_indices]\n",
    "y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "print(\"在样本数据上进行参数搜索...\")\n",
    "grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(\"最佳参数:\")\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最佳随机森林模型\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=400,           # 来自网格搜索结果\n",
    "    max_depth=20,               # 来自网格搜索结果\n",
    "    min_samples_split=10,       # 来自网格搜索结果\n",
    "    min_samples_leaf=4,         # 来自网格搜索结果\n",
    "    max_features='sqrt',        # 来自网格搜索结果\n",
    "    random_state=42,\n",
    "    n_jobs=-1                   # 使用多核加速\n",
    ")\n",
    "\n",
    "# 创建包含预处理器和最佳模型的管道\n",
    "final_rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', best_rf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在完整训练集上训练最终模型\n",
    "print(\"在完整训练集上训练最终模型...\")\n",
    "final_rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 保存模型与参数字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 创建保存模型的目录（如果不存在）\n",
    "os.makedirs('../models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存完整的随机森林管道...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/final_rf_pipeline.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"保存完整的随机森林管道...\")\n",
    "joblib.dump(final_rf_pipeline, '../models/final_rf_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 4,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# 将最佳参数保存为pickle文件\n",
    "with open('../models/best_rf_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 测试集训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估模型\n",
    "y_pred_rf = final_rf_pipeline.predict(X_test)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "print(f\"最终随机森林模型 - R²: {r2_rf:.4f}, RMSE: {rmse_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Stacking集成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 定义基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=grid_search.best_params_['model__n_estimators'],\n",
    "        max_depth=grid_search.best_params_['model__max_depth'],\n",
    "        min_samples_split=grid_search.best_params_['model__min_samples_split'],\n",
    "        min_samples_leaf=grid_search.best_params_['model__min_samples_leaf'],\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('gbm', GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('xgb', xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )),\n",
    "    ('lgbm', lgb.LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Ridge 作为最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking = StackingRegressor(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=Ridge(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# 创建完整的Stacking管道\n",
    "stacking_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('stacking', stacking)\n",
    "])\n",
    "\n",
    "# 训练Stacking模型\n",
    "stacking_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估Stacking模型\n",
    "y_pred_stack = stacking_pipeline.predict(X_test)\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "rmse_stack = np.sqrt(mean_squared_error(y_test, y_pred_stack))\n",
    "\n",
    "print(f\"Stacking集成 - R²: {r2_stack:.4f}, RMSE: {rmse_stack:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mixrec)",
   "language": "python",
   "name": "mixrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
